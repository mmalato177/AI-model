{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODELO S**\n",
    "Sem data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importações e Definições de Seed**\n",
    "\n",
    "Primeiro, importamos as bibliotecas necessárias e definimos a seed para garantir a reprodutibilidade dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definição dos Diretórios do Dataset e Tamanho da Imagem**\n",
    "\n",
    "Definimos os diretórios dos conjuntos de dados e o tamanho das imagens.\n",
    "Juntamos as imagens de treino todas na mesma pastas para simplificar o carregamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../Data/train'\n",
    "validation_dir = '../Data/validation5'\n",
    "test_dir = '../Data/test'\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para Visualizaçao de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, test_number, trial_number):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../plots/s{test_number}/trial_{trial_number}_plot.png')\n",
    "\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def predict_with_best_model(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        y_true.extend(tf.argmax(y, axis=1))\n",
    "        y_pred.extend(tf.argmax(model.predict(x), axis=1))\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Função para plotar matriz de confusão\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, test_number,trial_number):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'../plots/s{test_number}/trial_{trial_number}_confusion_matrix.png')\n",
    "\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carregamento dos Conjuntos de Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 files belonging to 10 classes.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Found 10000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalização dos Conjuntos de Dados**\n",
    "\n",
    "Normalizamos os conjuntos de dados para garantir que os valores dos pixels estejam na faixa de 0 a 1 o que são mais adequados ao modelo de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: (normalization_layer(x), y)\n",
    ")\n",
    "\n",
    "validation_dataset = validation_dataset.map(\n",
    "    lambda x, y: (normalization_layer(x), y)\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda x, y: (normalization_layer(x), y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introdução à Otimização de Hiperparâmetros com Optuna**\n",
    "\n",
    "A otimização de hiperparâmetros é crucial para desenvolver modelos de machine learning eficazes. Com o Optuna, uma biblioteca poderosa de otimização automática, podemos explorar eficientemente o espaço de hiperparâmetros para encontrar as melhores configurações. \n",
    "Integrar o Optuna ao nosso projeto automatiza o ajuste fino dos hiperparâmetros da nossa CNN, garantindo um modelo mais preciso e robusto. Não só acelera a experimentação, mas também maximiza o desempenho do modelo, contribuindo significativamente para o sucesso do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Função para Criar o Modelo de CNN**\n",
    "\n",
    "Definimos a função `create_model`, que cria um modelo de rede neural convolucional com base nos hiperparâmetros fornecidos.\n",
    "\n",
    "`Dropout` inserido com taxa de 0.3 após as camadas convolucionais e 0.5 após as camadas fully connected para regularizar o modelo, prevenindo overfitting ao desligar aleatoriamente unidades durante o treinamento.\n",
    "\n",
    "Obs: Na terceira Camada optamos por não fazer o `MaxPooling2D` pois os filtros ficariam muito pequenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    num_filters_1,\n",
    "    num_filters_2,\n",
    "    num_filters_3,\n",
    "    optimizer_name,\n",
    "    learning_rate,\n",
    "    momentum,\n",
    "    img_size\n",
    "):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(img_size, img_size, 3)))\n",
    "\n",
    "    # Primeira camada convolucional\n",
    "    model.add(layers.Conv2D(num_filters_1, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Segunda camada convolucional\n",
    "    model.add(layers.Conv2D(num_filters_2, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Terceira camada convolucional\n",
    "    model.add(layers.Conv2D(num_filters_3, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Camadas flatten e dense\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compilar o modelo com o otimizador especificado\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = getattr(optimizers, optimizer_name)(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo da estrutura Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">287,562</span> (1.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m287,562\u001b[0m (1.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">287,370</span> (1.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m287,370\u001b[0m (1.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_filters_1 = 32\n",
    "num_filters_2 = 32\n",
    "num_filters_3 = 32\n",
    "optimizer_name = 'Adam'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "\n",
    "# Primeira camada convolucional\n",
    "model.add(layers.Conv2D(num_filters_1, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Segunda camada convolucional\n",
    "model.add(layers.Conv2D(num_filters_2, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Terceira camada convolucional\n",
    "model.add(layers.Conv2D(num_filters_3, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Camadas flatten e dense\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = getattr(optimizers, optimizer_name)(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Função Objetivo para Otimização**\n",
    "\n",
    "A função `objective` é utilizada na otimização dos hiperparâmetros do modelo utilizando o Optuna. Nesta função, sugerimos diferentes valores para ajustar os parâmetros da rede neural convolucional (CNN), visando melhorar seu desempenho.\n",
    "\n",
    "Nesta configuração, exploramos diferentes números de filtros para as camadas convolucionais e opções de otimizadores como Adam ou SGD. Além disso, ajustamos a taxa de aprendizado de forma logarítmica para uma busca mais eficiente no espaço de hiperparâmetros. Essa abordagem nos permite encontrar combinações ideais que maximizem a precisão e a robustez do nosso modelo de CNN.\n",
    "\n",
    "- ***num_filters_1***, ***num_filters_2***, ***num_filters_3***\n",
    "\n",
    "  - Determinam o número de filtros nas camadas convolucionais da rede neural. Com opções de [32, 64], você está testando dois conjuntos de valores para cada camada convolucional. Um valor mais baixo como 32 pode ser adequado para capturar características mais simples, enquanto 64 pode capturar características mais complexas. Testar ambos permite avaliar qual configuração resulta em melhores resultados de desempenho e generalização para sua tarefa específica.\n",
    "\n",
    "- ***optimizer_name***\n",
    "\n",
    "  - Define o otimizador a ser usado durante o treinamento da rede neural. As opções 'Adam' e 'SGD' oferecem escolhas entre um otimizador adaptativo comumente usado (Adam) e um otimizador estocástico de gradiente descendente (SGD). O que influencia a velocidade e a precisão da convergência durante o treinamento.\n",
    "\n",
    "- ***learning_rate***\n",
    "\n",
    "  - Usar uma distribuição loguniforme entre 1e-4 e 1e-3 permite testar uma ampla gama de valores de taxa de aprendizado. Valores menores podem ajudar na convergência, enquanto valores maiores podem acelerar o treinamento inicial, mas podem ser menos estáveis.\n",
    "\n",
    "- ***Callbacks***\n",
    "  - `ModelCheckpoint`\n",
    "      - Guarda o melhor modelo de cada trail.\n",
    "\n",
    "  - `EarlyStopping`\n",
    "      - Reduz a taxa de aprendizado do modelo quando a métrica 'val_loss' para de melhorar, ajudando a garantir que o treinamento seja estável e eficaz.\n",
    "\n",
    "  - `ReduceLROnPlateau`\n",
    "    - Interrompe o treinamento quando a métrica 'val_loss' para de melhorar por um número especificado de épocas (***patience***), restaurando os melhores pesos obtidos durante o treinamento.\n",
    "\n",
    "Os parâmetros foram selecionados para equilibrar eficiência computacional, capacidade de aprendizado e generalização do modelo, seguindo práticas comuns em redes neurais convolucionais para garantir desempenho aceitavel em tarefas de classificação de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Estabelecer os valores sugeridos para cada hiperparâmetro\n",
    "    num_filters_1 = trial.suggest_categorical('num_filters_1', [32, 64])\n",
    "    num_filters_2 = trial.suggest_categorical('num_filters_2', [32, 64])\n",
    "    num_filters_3 = trial.suggest_categorical('num_filters_3', [32, 64])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-3, 1e-4])\n",
    "    momentum = 0.9\n",
    "    \n",
    "    model = create_model(\n",
    "        num_filters_1, \n",
    "        num_filters_2, \n",
    "        num_filters_3,\n",
    "        optimizer_name, \n",
    "        learning_rate,\n",
    "        momentum,\n",
    "        IMG_SIZE\n",
    "    )\n",
    "\n",
    "    # Definir callbacks, incluindo ModelCheckpoint para salvar o melhor modelo\n",
    "    model_dir = '../models/s1/'\n",
    "    trial_number = str(trial.number)\n",
    "    model_path = model_dir + 'model_trial_' + trial_number + '_CPbest.keras'\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        #   Uma vez que o modelo esta a optimizar baseado em accuracy aqui \n",
    "        # colocamos em relação ao loss para ter mais parametros a ajudar \n",
    "        # a aproximar do melhor modelo.\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.2, \n",
    "            patience=3, \n",
    "            min_lr=1e-5\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    plot_training_history(history, '1', trial_number)\n",
    "    \n",
    "    return history.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Otimização do Modelo**\n",
    "\n",
    "numero de trails =  2 (num_filters_1) × 2 (num_filters_2) × 2 (num_filters_3) × 2 (otimizadores) × 2 (learning_rate)  = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:29:44,169] A new study created in memory with name: no-name-f2b0a884-9144-4be3-886b-4950bd39f407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.92279, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.92279 to 1.91952, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.91952 to 1.64900, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.64900 to 1.61996, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.61996 to 1.55388, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.55388 to 1.51339, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.51339 to 1.49111, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.49111 to 1.49050, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.49050 to 1.46203, saving model to ../models/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.46203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:32:52,791] Trial 0 finished with value: 1.4938578605651855 and parameters: {'num_filters_1': 64, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'SGD', 'learning_rate': 0.0001}. Best is trial 0 with value: 1.4938578605651855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.45938, saving model to ../models/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.45938 to 1.36350, saving model to ../models/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.36350\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.36350\n",
      "\n",
      "Epoch 5: val_loss improved from 1.36350 to 1.22301, saving model to ../models/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.22301\n",
      "\n",
      "Epoch 7: val_loss improved from 1.22301 to 1.10295, saving model to ../models/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.10295\n",
      "\n",
      "Epoch 9: val_loss improved from 1.10295 to 1.09825, saving model to ../models/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.09825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:35:59,995] Trial 1 finished with value: 1.1705812215805054 and parameters: {'num_filters_1': 64, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'SGD', 'learning_rate': 0.001}. Best is trial 1 with value: 1.1705812215805054.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.59909, saving model to ../models/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.59909 to 1.41746, saving model to ../models/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.41746\n",
      "\n",
      "Epoch 4: val_loss improved from 1.41746 to 1.24887, saving model to ../models/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.24887\n",
      "\n",
      "Epoch 6: val_loss improved from 1.24887 to 1.20413, saving model to ../models/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.20413\n",
      "\n",
      "Epoch 8: val_loss improved from 1.20413 to 1.19384, saving model to ../models/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.19384\n",
      "\n",
      "Epoch 10: val_loss improved from 1.19384 to 1.17582, saving model to ../models/model_trial_2_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:39:07,633] Trial 2 finished with value: 1.1758157014846802 and parameters: {'num_filters_1': 32, 'num_filters_2': 64, 'num_filters_3': 64, 'optimizer': 'SGD', 'learning_rate': 0.001}. Best is trial 1 with value: 1.1705812215805054.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.44758, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.44758\n",
      "\n",
      "Epoch 3: val_loss improved from 1.44758 to 1.38613, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.38613 to 1.05453, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.05453 to 1.03482, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.03482 to 0.99998, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 0.99998 to 0.87130, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.87130 to 0.84235, saving model to ../models/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.84235\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.84235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:42:17,278] Trial 3 finished with value: 0.8584625124931335 and parameters: {'num_filters_1': 32, 'num_filters_2': 64, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 3 with value: 0.8584625124931335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.51654, saving model to ../models/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.51654 to 1.51064, saving model to ../models/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.51064 to 1.30220, saving model to ../models/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.30220\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.30220\n",
      "\n",
      "Epoch 6: val_loss improved from 1.30220 to 1.19751, saving model to ../models/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.19751\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.19751\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.19751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:45:05,014] Trial 4 finished with value: 1.4459023475646973 and parameters: {'num_filters_1': 64, 'num_filters_2': 64, 'num_filters_3': 64, 'optimizer': 'SGD', 'learning_rate': 0.001}. Best is trial 3 with value: 0.8584625124931335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.99795, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.99795 to 1.80225, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.80225 to 1.70898, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.70898\n",
      "\n",
      "Epoch 5: val_loss improved from 1.70898 to 1.58774, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.58774 to 1.52313, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.52313\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.52313\n",
      "\n",
      "Epoch 9: val_loss improved from 1.52313 to 1.46653, saving model to ../models/model_trial_5_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.46653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:48:12,193] Trial 5 finished with value: 1.547646403312683 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'SGD', 'learning_rate': 0.0001}. Best is trial 3 with value: 0.8584625124931335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.01763, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 2.01763 to 1.77048, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.77048 to 1.64180, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.64180 to 1.56112, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.56112 to 1.52330, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.52330\n",
      "\n",
      "Epoch 7: val_loss improved from 1.52330 to 1.48575, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.48575 to 1.48479, saving model to ../models/model_trial_6_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.48479\n",
      "\n",
      "Epoch 10: val_loss improved from 1.48479 to 1.40937, saving model to ../models/model_trial_6_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:51:18,008] Trial 6 finished with value: 1.409365177154541 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'SGD', 'learning_rate': 0.0001}. Best is trial 3 with value: 0.8584625124931335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.48789, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.48789 to 1.30459, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30459 to 1.12664, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.12664 to 1.08590, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.08590\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08590 to 0.96101, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.96101\n",
      "\n",
      "Epoch 8: val_loss improved from 0.96101 to 0.80714, saving model to ../models/model_trial_7_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.80714\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.80714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:54:25,588] Trial 7 finished with value: 0.8536289930343628 and parameters: {'num_filters_1': 32, 'num_filters_2': 64, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 7 with value: 0.8536289930343628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.32610, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.32610 to 1.25002, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.25002 to 1.12792, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.12792\n",
      "\n",
      "Epoch 5: val_loss improved from 1.12792 to 1.04777, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.04777 to 0.96733, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.96733\n",
      "\n",
      "Epoch 8: val_loss improved from 0.96733 to 0.89451, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.89451 to 0.87630, saving model to ../models/model_trial_8_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.87630 to 0.80725, saving model to ../models/model_trial_8_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 00:57:33,830] Trial 8 finished with value: 0.8072530031204224 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.69741, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.69741 to 1.53660, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.53660 to 1.44404, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.44404 to 1.34847, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.34847 to 1.33408, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.33408\n",
      "\n",
      "Epoch 7: val_loss improved from 1.33408 to 1.26694, saving model to ../models/model_trial_9_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.26694\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.26694\n",
      "\n",
      "Epoch 10: val_loss improved from 1.26694 to 1.18830, saving model to ../models/model_trial_9_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:00:41,563] Trial 9 finished with value: 1.1882990598678589 and parameters: {'num_filters_1': 64, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.0001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.46479, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.46479 to 1.21898, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.21898\n",
      "\n",
      "Epoch 4: val_loss improved from 1.21898 to 1.17277, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.17277 to 1.13171, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.13171\n",
      "\n",
      "Epoch 7: val_loss improved from 1.13171 to 0.93668, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.93668\n",
      "\n",
      "Epoch 9: val_loss improved from 0.93668 to 0.84756, saving model to ../models/model_trial_10_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.84756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:03:48,051] Trial 10 finished with value: 0.8523767590522766 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.46123, saving model to ../models/model_trial_11_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.46123 to 1.42045, saving model to ../models/model_trial_11_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.42045\n",
      "\n",
      "Epoch 4: val_loss improved from 1.42045 to 1.05654, saving model to ../models/model_trial_11_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.05654\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.05654\n",
      "\n",
      "Epoch 7: val_loss improved from 1.05654 to 0.94726, saving model to ../models/model_trial_11_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.94726\n",
      "\n",
      "Epoch 9: val_loss improved from 0.94726 to 0.84948, saving model to ../models/model_trial_11_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.84948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:06:54,200] Trial 11 finished with value: 0.8630514740943909 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.48223, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.48223 to 1.29531, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.29531\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.29531\n",
      "\n",
      "Epoch 5: val_loss improved from 1.29531 to 1.18289, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.18289 to 1.01295, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.01295\n",
      "\n",
      "Epoch 8: val_loss improved from 1.01295 to 0.89929, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.89929 to 0.87985, saving model to ../models/model_trial_12_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.87985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:10:01,925] Trial 12 finished with value: 0.9670686721801758 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.43545, saving model to ../models/model_trial_13_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.43545 to 1.25581, saving model to ../models/model_trial_13_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.25581 to 1.18124, saving model to ../models/model_trial_13_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.18124 to 1.13526, saving model to ../models/model_trial_13_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.13526\n",
      "\n",
      "Epoch 6: val_loss improved from 1.13526 to 0.93397, saving model to ../models/model_trial_13_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.93397\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.93397\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.93397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:12:50,142] Trial 13 finished with value: 1.0514543056488037 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.50075, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.50075 to 1.34491, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.34491 to 1.24443, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.24443 to 1.10144, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.10144 to 0.95944, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.95944\n",
      "\n",
      "Epoch 7: val_loss improved from 0.95944 to 0.92365, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.92365 to 0.92025, saving model to ../models/model_trial_14_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.92025\n",
      "\n",
      "Epoch 10: val_loss improved from 0.92025 to 0.89612, saving model to ../models/model_trial_14_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:15:57,557] Trial 14 finished with value: 0.8961184620857239 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.52400, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.52400 to 1.30098, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30098 to 1.25013, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.25013 to 1.10443, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.10443\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.10443\n",
      "\n",
      "Epoch 7: val_loss improved from 1.10443 to 1.02511, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.02511 to 0.86354, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.86354 to 0.84753, saving model to ../models/model_trial_15_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.84753 to 0.84457, saving model to ../models/model_trial_15_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:19:03,562] Trial 15 finished with value: 0.8445688486099243 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.58384, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.58384 to 1.52532, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.52532 to 1.13142, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.13142 to 1.04449, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.04449\n",
      "\n",
      "Epoch 6: val_loss improved from 1.04449 to 1.01502, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.01502 to 0.90739, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.90739 to 0.90354, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.90354 to 0.85788, saving model to ../models/model_trial_16_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.85788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:22:10,694] Trial 16 finished with value: 0.9276623725891113 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.50957, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.50957 to 1.39873, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.39873 to 1.19348, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.19348 to 1.17114, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.17114 to 1.16546, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.16546 to 0.95829, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 0.95829 to 0.95310, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.95310\n",
      "\n",
      "Epoch 9: val_loss improved from 0.95310 to 0.95096, saving model to ../models/model_trial_17_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.95096 to 0.89211, saving model to ../models/model_trial_17_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:25:17,520] Trial 17 finished with value: 0.8921107649803162 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.59986, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.59986 to 1.41890, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.41890\n",
      "\n",
      "Epoch 4: val_loss improved from 1.41890 to 1.28099, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.28099 to 1.27024, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.27024\n",
      "\n",
      "Epoch 7: val_loss improved from 1.27024 to 1.25638, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.25638\n",
      "\n",
      "Epoch 9: val_loss improved from 1.25638 to 1.16612, saving model to ../models/model_trial_18_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.16612 to 1.14929, saving model to ../models/model_trial_18_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:28:25,831] Trial 18 finished with value: 1.149288296699524 and parameters: {'num_filters_1': 64, 'num_filters_2': 64, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.0001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.48616, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.48616 to 1.30101, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30101 to 1.13303, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.13303 to 1.10288, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.10288 to 1.05439, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.05439\n",
      "\n",
      "Epoch 7: val_loss improved from 1.05439 to 0.91726, saving model to ../models/model_trial_19_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.91726\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.91726\n",
      "\n",
      "Epoch 10: val_loss improved from 0.91726 to 0.85072, saving model to ../models/model_trial_19_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:31:32,478] Trial 19 finished with value: 0.8507242202758789 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.55126, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.55126 to 1.25315, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.25315 to 1.18035, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.18035 to 1.01942, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.01942\n",
      "\n",
      "Epoch 6: val_loss improved from 1.01942 to 0.93688, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 0.93688 to 0.91622, saving model to ../models/model_trial_20_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.91622\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.91622\n",
      "\n",
      "Epoch 10: val_loss improved from 0.91622 to 0.84965, saving model to ../models/model_trial_20_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:34:38,410] Trial 20 finished with value: 0.8496494889259338 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.41540, saving model to ../models/model_trial_21_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.41540\n",
      "\n",
      "Epoch 3: val_loss improved from 1.41540 to 1.13540, saving model to ../models/model_trial_21_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.13540\n",
      "\n",
      "Epoch 5: val_loss improved from 1.13540 to 1.00737, saving model to ../models/model_trial_21_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.00737\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.00737\n",
      "\n",
      "Epoch 8: val_loss improved from 1.00737 to 1.00153, saving model to ../models/model_trial_21_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.00153 to 0.86698, saving model to ../models/model_trial_21_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.86698 to 0.85481, saving model to ../models/model_trial_21_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:37:44,264] Trial 21 finished with value: 0.8548123836517334 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.50757, saving model to ../models/model_trial_22_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.50757 to 1.21613, saving model to ../models/model_trial_22_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.21613\n",
      "\n",
      "Epoch 4: val_loss improved from 1.21613 to 1.09197, saving model to ../models/model_trial_22_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09197 to 0.98700, saving model to ../models/model_trial_22_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.98700\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.98700\n",
      "\n",
      "Epoch 8: val_loss improved from 0.98700 to 0.87098, saving model to ../models/model_trial_22_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.87098\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.87098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:40:50,039] Trial 22 finished with value: 0.9142076373100281 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.57593, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.57593 to 1.25947, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.25947 to 1.23433, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.23433 to 1.21269, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.21269 to 1.10608, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.10608 to 1.00873, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.00873 to 0.96940, saving model to ../models/model_trial_23_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.96940\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.96940\n",
      "\n",
      "Epoch 10: val_loss improved from 0.96940 to 0.89664, saving model to ../models/model_trial_23_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:43:56,564] Trial 23 finished with value: 0.8966420888900757 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.47187, saving model to ../models/model_trial_24_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.47187 to 1.20940, saving model to ../models/model_trial_24_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.20940 to 1.20049, saving model to ../models/model_trial_24_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.20049 to 1.05903, saving model to ../models/model_trial_24_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.05903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.05903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.05903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:46:09,264] Trial 24 finished with value: 1.1329811811447144 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.60665, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.60665 to 1.25441, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.25441 to 1.12738, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.12738 to 1.11822, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.11822\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.11822\n",
      "\n",
      "Epoch 7: val_loss improved from 1.11822 to 0.91649, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.91649 to 0.87421, saving model to ../models/model_trial_25_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.87421\n",
      "\n",
      "Epoch 10: val_loss improved from 0.87421 to 0.86840, saving model to ../models/model_trial_25_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:49:15,104] Trial 25 finished with value: 0.8683971762657166 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.65493, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.65493 to 1.44190, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.44190 to 1.36519, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.36519 to 1.34908, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.34908\n",
      "\n",
      "Epoch 6: val_loss improved from 1.34908 to 1.28174, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.28174 to 1.20243, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.20243 to 1.09841, saving model to ../models/model_trial_26_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.09841\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.09841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:52:22,416] Trial 26 finished with value: 1.1191442012786865 and parameters: {'num_filters_1': 64, 'num_filters_2': 64, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.0001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.36386, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.36386\n",
      "\n",
      "Epoch 3: val_loss improved from 1.36386 to 1.15364, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.15364 to 1.08806, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.08806\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08806 to 0.99280, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 0.99280 to 0.94031, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.94031 to 0.89681, saving model to ../models/model_trial_27_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.89681\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.89681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:55:28,310] Trial 27 finished with value: 0.926841676235199 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.33950, saving model to ../models/model_trial_28_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.33950 to 1.18333, saving model to ../models/model_trial_28_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.18333\n",
      "\n",
      "Epoch 4: val_loss improved from 1.18333 to 1.10307, saving model to ../models/model_trial_28_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.10307\n",
      "\n",
      "Epoch 6: val_loss improved from 1.10307 to 1.00637, saving model to ../models/model_trial_28_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.00637\n",
      "\n",
      "Epoch 8: val_loss improved from 1.00637 to 0.82484, saving model to ../models/model_trial_28_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.82484\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.82484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 01:58:35,468] Trial 28 finished with value: 0.8615323305130005 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 64, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.91442, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.91442 to 1.83813, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.83813 to 1.62423, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.62423 to 1.61891, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.61891 to 1.59131, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.59131 to 1.55236, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.55236\n",
      "\n",
      "Epoch 8: val_loss improved from 1.55236 to 1.53180, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.53180 to 1.44767, saving model to ../models/model_trial_29_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.44767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 02:01:40,388] Trial 29 finished with value: 1.5980913639068604 and parameters: {'num_filters_1': 64, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'SGD', 'learning_rate': 0.0001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.51509, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.51509 to 1.43308, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.43308 to 1.14382, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.14382\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.14382\n",
      "\n",
      "Epoch 6: val_loss improved from 1.14382 to 1.11738, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.11738 to 1.05754, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.05754 to 1.01211, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.01211 to 0.90329, saving model to ../models/model_trial_30_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.90329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 02:04:46,224] Trial 30 finished with value: 0.9787349700927734 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.38603, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.38603\n",
      "\n",
      "Epoch 3: val_loss improved from 1.38603 to 1.15574, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.15574 to 1.08317, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08317 to 0.99886, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.99886\n",
      "\n",
      "Epoch 7: val_loss improved from 0.99886 to 0.99254, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.99254 to 0.93782, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.93782 to 0.90749, saving model to ../models/model_trial_31_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.90749 to 0.82351, saving model to ../models/model_trial_31_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 02:07:52,525] Trial 31 finished with value: 0.8235142827033997 and parameters: {'num_filters_1': 32, 'num_filters_2': 32, 'num_filters_3': 32, 'optimizer': 'Adam', 'learning_rate': 0.001}. Best is trial 8 with value: 0.8072530031204224.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Trial **0** \n",
    "  - Perda de Validação: 1.4938578605651855 \n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 32, Layer 3: 32, Optimizer: SGD, Learning Rate: 0.0001}\n",
    "\n",
    "  \n",
    "![](../plots/s1/trial_0_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **1** \n",
    "  \n",
    "  - Perda de Validação: 1.1705812215805054\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 32, Layer 3: 64, Optimizer: SGD, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_1_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **2** \n",
    "\n",
    "  - Perda de Validação: 1.1758157014846802\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 64, Layer 3: 64, Optimizer: SGD, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_2_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **3** \n",
    "\n",
    "  - Perda de Validação: 0.8584625124931335\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 64, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "  \n",
    "![](../plots/s1/trial_3_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **4** \n",
    "  - Perda de Validação: 1.4459023475646973\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 64, Layer 3: 64, Optimizer: SGD, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_4_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **5** \n",
    "\n",
    "  - Perda de Validação: 1.547646403312683\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: SGD, Learning Rate: 0.0001}\n",
    "  \n",
    "![](../plots/s1/trial_5_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **6** \n",
    "\n",
    "  - Perda de Validação: 1.409365177154541\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: SGD, Learning Rate: 0.0001}\n",
    "  \n",
    "![](../plots/s1/trial_6_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **7**\n",
    "\n",
    "  - Perda de Validação: 0.8536289930343628\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 64, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.001}\n",
    "  \n",
    "![](../plots/s1/trial_7_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **8**\n",
    "  \n",
    "  - Perda de Validação: 0.8072530031204224\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.001}\n",
    "  \n",
    "![](../plots/s1/trial_8_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **9** \n",
    "  - Perda de Validação: 1.1882990598678589\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.0001}\n",
    "\n",
    "![](../plots/s1/trial_9_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **10** \n",
    "  - Perda de Validação: 0.8523767590522766\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_10_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **11** \n",
    "  - Perda de Validação: 0.8630514740943909\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_11_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **12** \n",
    "  - Perda de Validação: 0.9670686721801758\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_12_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **13** \n",
    "  - Perda de Validação: 1.0514543056488037\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_13_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **14** \n",
    "  - Perda de Validação: 0.8961184620857239\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_14_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **15** \n",
    "  - Perda de Validação: 0.8445688486099243\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_15_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **16** \n",
    "  - Perda de Validação: 0.9276623725891113\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_16_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **17**\n",
    "\n",
    "  - Perda de Validação: 0.8921107649803162\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_17_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **18**\n",
    "\n",
    "  - Perda de Validação: 1.149288296699524\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 64, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.0001}\n",
    "\n",
    "![](../plots/s1/trial_18_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **19**\n",
    "\n",
    "  - Perda de Validação: 0.8507242202758789\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_19_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **20**\n",
    "\n",
    "  - Perda de Validação: 0.8496494889259338\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_20_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **21**\n",
    "\n",
    "  - Perda de Validação: 0.8548123836517334\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_21_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **22**\n",
    "\n",
    "  - Perda de Validação: 0.9142076373100281\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_22_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **23**\n",
    "\n",
    "  - Perda de Validação: 0.8966420888900757\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_23_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **24**\n",
    "\n",
    "  - Perda de Validação: 1.1329811811447144\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_24_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **25**\n",
    "\n",
    "  - Perda de Validação: 0.8683971762657166\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_25_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **26**\n",
    "\n",
    "  - Perda de Validação: 1.1191442012786865\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 64, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.0001}\n",
    "\n",
    "![](../plots/s1/trial_26_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **27**\n",
    "\n",
    "  - Perda de Validação: 0.926841676235199\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_27_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **28**\n",
    "\n",
    "  - Perda de Validação: 0.8615323305130005\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 64, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_28_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **29** \n",
    "\n",
    "  - Perda de Validação: 1.5980913639068604\n",
    "\n",
    "  - {Layer 1: 64, Layer 2: 32, Layer 3: 32, Optimizer: SGD, Learning Rate: 0.0001}\n",
    "\n",
    "![](../plots/s1/trial_29_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **30** \n",
    "\n",
    "  - Perda de Validação: 0.9787349700927734\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_30_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **31** \n",
    "\n",
    "  - Perda de Validação: 0.8235142827033997\n",
    "\n",
    "  - {Layer 1: 32, Layer 2: 32, Layer 3: 32, Optimizer: Adam, Learning Rate: 0.001}\n",
    "\n",
    "![](../plots/s1/trial_31_plot.png) \n",
    "  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualização dos Resultados**\n",
    "\n",
    "### Melhor modelo **8** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o melhor modelo encontrado\n",
    "best_model_path ='../models/s1/model_trial_8_CPbest.keras'\n",
    "\n",
    "best_model = models.load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7189 - loss: 0.8086\n",
      "Precião no conjunto de Teste: 0.7124\n",
      "Perda no conjunto de Teste: 0.8212\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o melhor modelo no conjunto de teste\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(f'Precião no conjunto de Teste: {test_acc:.4f}')\n",
    "print(f'Perda no conjunto de Teste: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719074295.574267   18933 service.cc:145] XLA service 0x7f2f840019f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1719074295.574317   18933 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-06-22 17:38:15.601839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1719074296.545963   18933 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 17:38:38.874421: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusão\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_true, y_pred = predict_with_best_model(best_model, test_dataset)\n",
    "\n",
    "class_names = ['Avião', 'Automovel', 'Passaro', 'Gato', 'Viado',\n",
    "                'Cão', 'Sapo', 'Cavalo', 'Barco', 'Camião']\n",
    "plot_confusion_matrix(y_true, y_pred, class_names, '1', '8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de Confusão do modelo **8**\n",
    "\n",
    "Na Matriz de confusão poderos reparar que dentro de categorias que são animais e maquinas existem mais falhas. \n",
    "\n",
    "Isto ocorre devido à semelhança entre eles por exemplo as cores, linhas mais retas, etc. \n",
    "\n",
    "![](../plots/s1/trial_8_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecionar Modelos Interessantes\n",
    "\n",
    "Dos modelos criados anteriormente, embora tenham sido repedidos alguns parametros e tenham sido feitas poucas épocas podemos tirar algumas conclusões.\n",
    "Os modelos que mostraram resultados mais interessantes foram o 3, 7, 8 e 15.\n",
    "\n",
    "- **Filtros por Layer**\n",
    "    - Regra geral apenas os modelos com 32 filtros no primeiro layer tiveram bom resultados.\n",
    "    - Os valores utilizados nos layers 2 e 3 não parecem influenciar muito no resultado, mas o melhor fui 32, 64 respetivamente então vamos guardar esses parametros para o modelo final.\n",
    "\n",
    "- **Optimizador e Learning Rate**\n",
    "    - O optimizador `Adaptive Moment Estimation` foi o que gerou melhores resultados. De seguida iremos experimentar os mais valores de learning rate para os modelos.\n",
    "\n",
    "Vamos também aumentar o número de epocas uma vez que os graficos não mostram sinais de overfitting para estes modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_filters_1 = 32\n",
    "    num_filters_2 = 32\n",
    "    num_filters_3 = 64\n",
    "    optimizer_name = 'Adam'\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-2, 5e-3, 3e-4, 1e-5, 5e-5])\n",
    "    momentum = None\n",
    "    \n",
    "    model = create_model(\n",
    "        num_filters_1, \n",
    "        num_filters_2, \n",
    "        num_filters_3,\n",
    "        optimizer_name, \n",
    "        learning_rate,\n",
    "        momentum,\n",
    "        IMG_SIZE\n",
    "    )\n",
    "    \n",
    "    model_dir = '../models/s2/'\n",
    "    trial_number = str(trial.number)\n",
    "    model_path = model_dir + 'model_trial_' + trial_number + '_CPbest.keras'\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.2, \n",
    "            patience=3, \n",
    "            min_lr=1e-5\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=20,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    plot_training_history(history, '2', trial_number)\n",
    "    \n",
    "    return history.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:04:28,467] A new study created in memory with name: no-name-b0b25ca8-a019-41ae-8e60-f26e36a59753\n",
      "/home/patinhooh/miniconda3/envs/ia/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718895870.759888   21209 service.cc:145] XLA service 0x7f7d3c001de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1718895870.759994   21209 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-06-20 16:04:30.921842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-20 16:04:31.437061: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1718895877.699153   21209 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.46992, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.46992\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.46992\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.46992\n",
      "\n",
      "Epoch 5: val_loss improved from 1.46992 to 0.96520, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 0.96520 to 0.89891, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.89891\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.89891\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.89891\n",
      "\n",
      "Epoch 10: val_loss improved from 0.89891 to 0.80748, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 0.80748 to 0.78331, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.78331\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.78331\n",
      "\n",
      "Epoch 14: val_loss improved from 0.78331 to 0.78065, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 15: val_loss improved from 0.78065 to 0.76011, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.76011\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.76011\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.76011\n",
      "\n",
      "Epoch 19: val_loss improved from 0.76011 to 0.75599, saving model to ../models/s2/model_trial_0_CPbest.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 0.75599 to 0.75469, saving model to ../models/s2/model_trial_0_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:11:29,313] Trial 0 finished with value: 0.75468909740448 and parameters: {'learning_rate': 0.005}. Best is trial 0 with value: 0.75468909740448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.48253, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.48253 to 1.36841, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.36841\n",
      "\n",
      "Epoch 4: val_loss improved from 1.36841 to 1.18898, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.18898\n",
      "\n",
      "Epoch 6: val_loss improved from 1.18898 to 1.18084, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.18084 to 0.95129, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.95129 to 0.91447, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.91447 to 0.86661, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.86661 to 0.84770, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.84770\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.84770\n",
      "\n",
      "Epoch 13: val_loss improved from 0.84770 to 0.79826, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.79826\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.79826\n",
      "\n",
      "Epoch 16: val_loss improved from 0.79826 to 0.74878, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 0.74878 to 0.73321, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 0.73321 to 0.70186, saving model to ../models/s2/model_trial_1_CPbest.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.70186\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.70186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:17:54,819] Trial 1 finished with value: 0.8625155091285706 and parameters: {'learning_rate': 0.0003}. Best is trial 0 with value: 0.75468909740448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.52193, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.52193 to 1.46944, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.46944 to 1.36553, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.36553 to 1.25812, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.25812\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.25812\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.25812\n",
      "\n",
      "Epoch 8: val_loss improved from 1.25812 to 1.00434, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.00434 to 0.93034, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.93034 to 0.86812, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 0.86812 to 0.85836, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.85836\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.85836\n",
      "\n",
      "Epoch 14: val_loss improved from 0.85836 to 0.84251, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 15: val_loss improved from 0.84251 to 0.83268, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.83268\n",
      "\n",
      "Epoch 17: val_loss improved from 0.83268 to 0.79411, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 0.79411 to 0.79128, saving model to ../models/s2/model_trial_2_CPbest.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.79128\n",
      "\n",
      "Epoch 20: val_loss improved from 0.79128 to 0.79072, saving model to ../models/s2/model_trial_2_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:24:21,964] Trial 2 finished with value: 0.7907170653343201 and parameters: {'learning_rate': 0.005}. Best is trial 0 with value: 0.75468909740448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.62718, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.62718 to 1.35559, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.35559 to 1.22014, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.22014\n",
      "\n",
      "Epoch 5: val_loss improved from 1.22014 to 1.07628, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.07628 to 1.01566, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.01566\n",
      "\n",
      "Epoch 8: val_loss improved from 1.01566 to 0.95026, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.95026\n",
      "\n",
      "Epoch 10: val_loss improved from 0.95026 to 0.92810, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.92810\n",
      "\n",
      "Epoch 12: val_loss improved from 0.92810 to 0.89211, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.89211\n",
      "\n",
      "Epoch 14: val_loss improved from 0.89211 to 0.85763, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.85763\n",
      "\n",
      "Epoch 16: val_loss improved from 0.85763 to 0.82837, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 0.82837 to 0.80656, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 0.80656 to 0.76978, saving model to ../models/s2/model_trial_3_CPbest.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.76978\n",
      "\n",
      "Epoch 20: val_loss improved from 0.76978 to 0.70829, saving model to ../models/s2/model_trial_3_CPbest.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:30:44,040] Trial 3 finished with value: 0.7082935571670532 and parameters: {'learning_rate': 0.0003}. Best is trial 3 with value: 0.7082935571670532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.55734, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.55734\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.55734\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.55734\n",
      "\n",
      "Epoch 5: val_loss improved from 1.55734 to 1.22165, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.22165 to 1.13440, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.13440\n",
      "\n",
      "Epoch 8: val_loss improved from 1.13440 to 1.09052, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.09052 to 1.04598, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.04598\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.04598\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04598 to 0.96659, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.96659\n",
      "\n",
      "Epoch 14: val_loss improved from 0.96659 to 0.90627, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 15: val_loss improved from 0.90627 to 0.89303, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.89303\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.89303\n",
      "\n",
      "Epoch 18: val_loss improved from 0.89303 to 0.83695, saving model to ../models/s2/model_trial_4_CPbest.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.83695\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.83695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-20 16:37:05,702] Trial 4 finished with value: 0.8736234307289124 and parameters: {'learning_rate': 0.01}. Best is trial 3 with value: 0.7082935571670532.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Trial **0** \n",
    "  \n",
    "  - Loss Validation: 0.75468909740448\n",
    "\n",
    "  - Learning Rate: 0.005\n",
    "\n",
    "![](../plots/s2/trial_0_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **1** \n",
    "  \n",
    "  - Loss Validation: 0.8625155091285706\n",
    "\n",
    "  - Learning Rate: 0.0003\n",
    "\n",
    "![](../plots/s2/trial_1_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **2** \n",
    "  \n",
    "  - Loss Validation: 0.7907170653343201\n",
    "\n",
    "  - Learning Rate: 0.005\n",
    "\n",
    "![](../plots/s2/trial_2_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **3** \n",
    "  \n",
    "  - Loss Validation: 0.7082935571670532\n",
    "\n",
    "  - Learning Rate: 0.0003\n",
    "\n",
    "![](../plots/s2/trial_3_plot.png) \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Trial **4** \n",
    "  \n",
    "  - Loss Validation: 0.8736234307289124\n",
    "\n",
    "  - Learning Rate: 0.01\n",
    "\n",
    "![](../plots/s2/trial_4_plot.png) \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar resultados dos testes de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:  0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7336 - loss: 0.7636\n",
      "\tPrecião no conjunto de Teste: 0.7304\n",
      "\tPerda no conjunto de Teste:   0.7758\n",
      "\n",
      "Model:  1\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7505 - loss: 0.7117\n",
      "\tPrecião no conjunto de Teste: 0.7476\n",
      "\tPerda no conjunto de Teste:   0.7260\n",
      "\n",
      "Model:  2\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7295 - loss: 0.7831\n",
      "\tPrecião no conjunto de Teste: 0.7175\n",
      "\tPerda no conjunto de Teste:   0.8123\n",
      "\n",
      "Model:  3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7604 - loss: 0.7014\n",
      "\tPrecião no conjunto de Teste: 0.7559\n",
      "\tPerda no conjunto de Teste:   0.7164\n",
      "\n",
      "Model:  4\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7165 - loss: 0.8225\n",
      "\tPrecião no conjunto de Teste: 0.7061\n",
      "\tPerda no conjunto de Teste:   0.8439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    model_path =f'../models/s2/model_trial_{i}_CPbest.keras'\n",
    "    print(\"\\nModel: \",i,)\n",
    "    model = models.load_model(model_path)\n",
    "    # Avaliar o melhor modelo no conjunto de teste\n",
    "    test_loss, test_acc = model.evaluate(test_dataset)\n",
    "    print(f'\\tPrecião no conjunto de Teste: {test_acc:.4f}')\n",
    "    print(f'\\tPerda no conjunto de Teste:   {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentar treinar mais o modelo **3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.76635, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.76635\n",
      "\n",
      "Epoch 3: val_loss improved from 0.76635 to 0.71462, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71462\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.71462\n",
      "\n",
      "Epoch 6: val_loss improved from 0.71462 to 0.70207, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 0.70207 to 0.70048, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.70048\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.70048\n",
      "\n",
      "Epoch 10: val_loss improved from 0.70048 to 0.69179, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.69179\n",
      "\n",
      "Epoch 12: val_loss improved from 0.69179 to 0.67346, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.67346\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.67346\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.67346\n",
      "\n",
      "Epoch 16: val_loss improved from 0.67346 to 0.65790, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 0.65790 to 0.64667, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 0.64667 to 0.63378, saving model to ../models/s2/model_trial_3_1_CPbest.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.63378\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.63378\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model('../models/s2/model_trial_3_CPbest.keras')\n",
    "model_path = '../models/s2/model_trial_3_1_CPbest.keras'\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.2, \n",
    "        patience=3, \n",
    "        min_lr=1e-5\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plot_training_history(history, '2', '3_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Model **3** \n",
    "  \n",
    "  - 40 epocas no total\n",
    "  - Perda de Validação: 0.63378\n",
    "\n",
    "![](../plots/s2/trial_3_1_plot.png) \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo 3 depois de mais épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7826 - loss: 0.6413\n",
      "\tPrecião no conjunto de Teste: 0.7795\n",
      "\tPerda no conjunto de Teste:   0.6486\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o melhor modelo no conjunto de teste\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'\\tPrecião no conjunto de Teste: {test_acc:.4f}')\n",
    "print(f'\\tPerda no conjunto de Teste:   {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pode ver o modelo ainda está melhorar ainda que ligeiramente. Então vamos tentar treinar durante mais algumas épocas com os callbacks um pouco diferentes.\n",
    "\n",
    "- `EarlyStopping`\n",
    "    - Vamos aumentar um pouco mais a *patience* para que ele tenha mais espaço para poder progredir.\n",
    "\n",
    "- `ReduceLROnPlateau`\n",
    "    - Reduzir *factor* para o learning rate reduzir um pouco mais rápidamente, e ajustar um pouco a *patience* para o aplicar.\n",
    "\n",
    "E como a perda de validação já se encontra em níveis bons vamos experimentar monitorar a precisão desta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.7747 - loss: 0.6315 - val_accuracy: 0.7704 - val_loss: 0.6810 - learning_rate: 6.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7753 - loss: 0.6338 - val_accuracy: 0.7776 - val_loss: 0.6477 - learning_rate: 6.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7791 - loss: 0.6221 - val_accuracy: 0.7795 - val_loss: 0.6481 - learning_rate: 6.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7831 - loss: 0.6155 - val_accuracy: 0.7805 - val_loss: 0.6394 - learning_rate: 6.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7789 - loss: 0.6204 - val_accuracy: 0.7842 - val_loss: 0.6284 - learning_rate: 6.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7830 - loss: 0.6129 - val_accuracy: 0.7796 - val_loss: 0.6438 - learning_rate: 6.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7846 - loss: 0.6134 - val_accuracy: 0.7849 - val_loss: 0.6339 - learning_rate: 6.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7845 - loss: 0.6110 - val_accuracy: 0.7824 - val_loss: 0.6438 - learning_rate: 6.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7851 - loss: 0.6112 - val_accuracy: 0.7861 - val_loss: 0.6253 - learning_rate: 6.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7840 - loss: 0.6145 - val_accuracy: 0.7777 - val_loss: 0.6555 - learning_rate: 6.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7895 - loss: 0.5948 - val_accuracy: 0.7887 - val_loss: 0.6213 - learning_rate: 6.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7883 - loss: 0.6004 - val_accuracy: 0.7847 - val_loss: 0.6394 - learning_rate: 6.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7840 - loss: 0.6147 - val_accuracy: 0.7855 - val_loss: 0.6316 - learning_rate: 6.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7926 - loss: 0.5961 - val_accuracy: 0.7892 - val_loss: 0.6205 - learning_rate: 6.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7871 - loss: 0.5987 - val_accuracy: 0.7861 - val_loss: 0.6281 - learning_rate: 6.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7947 - loss: 0.5868 - val_accuracy: 0.7866 - val_loss: 0.6235 - learning_rate: 6.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7903 - loss: 0.5963 - val_accuracy: 0.7844 - val_loss: 0.6381 - learning_rate: 6.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7896 - loss: 0.5923 - val_accuracy: 0.7778 - val_loss: 0.6520 - learning_rate: 6.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7913 - loss: 0.5883 - val_accuracy: 0.7839 - val_loss: 0.6395 - learning_rate: 6.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7940 - loss: 0.5793 - val_accuracy: 0.7888 - val_loss: 0.6252 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7931 - loss: 0.5868 - val_accuracy: 0.7871 - val_loss: 0.6264 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7912 - loss: 0.5807 - val_accuracy: 0.7873 - val_loss: 0.6244 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7953 - loss: 0.5743 - val_accuracy: 0.7899 - val_loss: 0.6225 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7954 - loss: 0.5777 - val_accuracy: 0.7878 - val_loss: 0.6233 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7980 - loss: 0.5680 - val_accuracy: 0.7890 - val_loss: 0.6246 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7951 - loss: 0.5698 - val_accuracy: 0.7901 - val_loss: 0.6201 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.7990 - loss: 0.5738 - val_accuracy: 0.7888 - val_loss: 0.6212 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7969 - loss: 0.5702 - val_accuracy: 0.7905 - val_loss: 0.6212 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7956 - loss: 0.5737 - val_accuracy: 0.7875 - val_loss: 0.6243 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7947 - loss: 0.5760 - val_accuracy: 0.7892 - val_loss: 0.6235 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.8008 - loss: 0.5661 - val_accuracy: 0.7908 - val_loss: 0.6184 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7967 - loss: 0.5767 - val_accuracy: 0.7899 - val_loss: 0.6208 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7967 - loss: 0.5687 - val_accuracy: 0.7890 - val_loss: 0.6211 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.7964 - loss: 0.5766 - val_accuracy: 0.7884 - val_loss: 0.6250 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7959 - loss: 0.5765 - val_accuracy: 0.7892 - val_loss: 0.6218 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.8018 - loss: 0.5675 - val_accuracy: 0.7907 - val_loss: 0.6214 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7973 - loss: 0.5722 - val_accuracy: 0.7903 - val_loss: 0.6210 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.7961 - loss: 0.5719 - val_accuracy: 0.7906 - val_loss: 0.6161 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.7996 - loss: 0.5668 - val_accuracy: 0.7898 - val_loss: 0.6218 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7966 - loss: 0.5750 - val_accuracy: 0.7894 - val_loss: 0.6184 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.7961 - loss: 0.5748 - val_accuracy: 0.7903 - val_loss: 0.6206 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model('../models/s2/model_trial_3_1_CPbest.keras')\n",
    "model_path = '../models/s2/model_trial_3_2_CPbest.keras'\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        verbose=0\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        patience=10\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy', \n",
    "        mode='max',\n",
    "        factor=0.1,  \n",
    "        patience=5,  \n",
    "        min_lr=1e-5\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,  \n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history(history, '2', '3_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 3\n",
    "  - 81 épocas no total\n",
    "  - Precisão de Validação: 0.7908\n",
    "\n",
    "![](../plots/s2/trial_3_2_plot.png) \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desempenho Geral do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 17:41:02.381085: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o melhor modelo no conjunto de teste\n",
    "model = models.load_model('../models/s3/model_trial_3_2_DA_CPbest.keras')\n",
    "y_test, pred = predict_with_best_model(model, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7433 - loss: 0.7408\n",
      "\tPrecião no conjunto de Teste: 0.7350\n",
      "\tPerda no conjunto de Teste:   0.7643\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'\\tPrecião no conjunto de Teste: {test_acc:.4f}')\n",
    "print(f'\\tPerda no conjunto de Teste:   {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precisão no conjunto de Teste: 73.5%\n",
    "- Perda no conjunto de Teste: 0.7643\n",
    "\n",
    "O modelo apresenta um desempenho razoável, mas ainda há espaço para melhorias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 73.5%\n",
      "\n",
      "Precision:\n",
      "\tMacro:    74.13%\n",
      "\tWeighted: 74.13%\n",
      "\n",
      "Recall:\n",
      "\tMacro:    73.5%\n",
      "\tWeighted: 73.5%\n",
      "\n",
      "\n",
      "F1-score:\n",
      "\tMacro:    72.67%\n",
      "\tWeighted: 72.67%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_true =  y_test, y_pred = pred)\n",
    "print(f'Accuracy : {np.round(acc*100,2)}%')\n",
    "\n",
    "print(\"\\nPrecision:\")   \n",
    "precision = precision_score(y_true =  y_test, y_pred = pred, average='macro')\n",
    "print(f'\\tMacro:    {np.round(precision*100,2)}%')\n",
    "\n",
    "precision = precision_score(y_true =  y_test, y_pred = pred, average='weighted')\n",
    "print(f'\\tWeighted: {np.round(precision*100,2)}%')\n",
    "\n",
    "print(\"\\nRecall:\")\n",
    "recall = recall_score(y_true =  y_test, y_pred = pred, average='macro')\n",
    "print(f'\\tMacro:    {np.round(recall*100,2)}%')\n",
    "\n",
    "recall = recall_score(y_true =  y_test, y_pred = pred, average='weighted')\n",
    "print(f'\\tWeighted: {np.round(recall*100,2)}%')\n",
    "print()\n",
    "\n",
    "print(\"\\nF1-score:\")\n",
    "f1 = f1_score(y_true =  y_test, y_pred = pred, average='macro')\n",
    "print(f'\\tMacro:    {np.round(f1*100,2)}%')\n",
    "\n",
    "f1 = f1_score(y_true =  y_test, y_pred = pred, average='weighted')\n",
    "print(f'\\tWeighted: {np.round(f1*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado Precisão:\n",
    "- Macro: 74.13%\n",
    "- Ponderada: 74.13%\n",
    "\n",
    "A precisão macro e weighted são iguais, indicando que as classes são balanceadas. A precisão de aproximadamente 74% mostra que, quando o modelo prevê uma classe como positiva, ele está correto em 74% dos casos.\n",
    "\n",
    "<br>\n",
    "\n",
    "Resultado Revocação:\n",
    "- Macro: 73.5%\n",
    "- Ponderada: 73.5%\n",
    "\n",
    "As métricas de precisão e recall são bastante equilibradas, o que é positivo. Ambas as versões macro e ponderada são similares, indicando que o modelo está tratando as classes de forma relativamente uniforme.\n",
    "\n",
    "<br>\n",
    "\n",
    "Resultado F1-Score:\n",
    "- Macro: 72.67%\n",
    "- Ponderada: 72.67%\n",
    "\n",
    "O F1-score é ligeiramente mais baixo que a precisão e o recall indica que pode haver um pequeno desequilíbrio entre precisão e recall. Ajustes adicionais, como balanceamento de classes ou alteração da função de perda, poderiam ajudar a melhorar esta métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Avião', 'Automovel', 'Passaro', 'Gato', 'Viado',\n",
    "                'Cão', 'Sapo', 'Cavalo', 'Barco', 'Camião']\n",
    "plot_confusion_matrix(y_test, pred, class_names, '2', '3_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../plots/s2/trial_3_2_confusion_matrix.png)\n",
    "\n",
    "- Neste resultado podemos ver que os quadrados entre as categoria animal e maquina já se encontram mais diluidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações finais\n",
    "\n",
    "Como o modelo começou a apresentar alguns sinais de ovefitting vamos ficar por aqui com este modelo S sem data augmentation.\n",
    "Utilizaremos o que descobrimos aqui no ajuste do modelo com data augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
